---
title: "Prediction of Survival Outcome"
author: "Carlos S Traynor"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette serves as an approach to the modelling of survival data in medical statistics. We will review the most relevant methods including adaptations to the use of high throughput genomic data. There are a vast number of packages focused in survival analysis and our goal is make a summary of the most useful functions and the best ways to combine them to obtain a meaningfull analysis.  

The main outcome in survival analysis is the time untill an event occurs. The difference between survival analysis and other statistical analysis is that some events are censored, or not observed because the actual event time is longer than the follow-up of the study.

We will need various packages that can be installed through the installation of predsurv package. In addition we will work with a simulated dataset,  which is a list containing a gene expression matrix and the survival outcome of an hypothetical experiment, you can check the simulation algorithm in data-raw.
```{r}
library(predsurv)
theme_set(theme_bw())
#data("survdata")#load data
```
Because this is a  simulated dataset we can skip various steps of data manipulation, however we want to review a first data exploration, for example the time to event distribution and the Kaplan and Meier plots. Besides, create a test and training splits. We will use a function of the package survdata.


```{r}
# data("survdata")
survdata <- surv_sim_data(N = 400, features = 200, CenRate = 1/10)
plot_km(survdata, time = os_months, status = os_deceased)
```

```{r}
fold = create_training_test_set(survdata, p = 0.8, status = os_deceased)
train = fold[["train"]]
test = fold[["test"]]
head(train[,1:6]); rm(fold)
```




We will first fit a subset selection method, for example Univariate:
```{r, results="hide", eval = FALSE, error = FALSE, warning=FALSE}
mod.uni = fun_train(train, fit = "Univariate", time = os_months, status = os_deceased)
uni.brier_score = pred_error(obj = mod.uni, fit = "Univariate", pred = "Brier", time = os_months, status = os_deceased)
plot_brier(uni.brier_score)
```

Calculate the c-index:
```{R}
uni.c_index = pred_error(obj = mod.uni, fit = "Univariate", pred = "c_index", status = os_deceased, time = os_months)
print(uni.c_index)
```

```{r}
uni.roc = pred_error(obj = mod.uni, fit = "Univariate", pred = "ROC",  status = os_deceased, time = os_months)
tdROC::plot.tdROC(uni.roc)
```

The forward step wise method can be fitted using the function stepAIC in the MASS package:
```{r, results="hide", eval = FALSE, error = FALSE, warning=FALSE}
mod.fs = fun_train(train, fit = "Stepwise")
fs.brier_score = pred_error(obj = mod.fs, fit = "Stepwise")
plot_brier(fs.brier_score)
```

Now we can obtain the cumulative prediction error curve or integrated Brier Score :
```{r}
ibrier = fun_ibrier_score(fs.brier_score)
print(ibrier)
```
```{r}
fs.roc = pred_error(obj = mod.fs, fit = "Stepwise", pred = "ROC",  status = os_deceased, time = os_months)
tdROC::plot.tdROC(fs.roc)
```

Another method is the dimension reduction obtained by using supervised principal components and partial least square regression. 

##Supervised Principal Components

```{r}
mod.pcr = fun_train(train, fit = "PCR")
pcr.brier_score = pred_error(obj = mod.pcr, fit = "PCR")
plot_brier(pcr.brier_score)
```

```{r}
pcr.roc = pred_error(obj = mod.pcr, fit = "PCR", pred = "ROC")
tdROC::plot.tdROC(pcr.roc)
```

```{r}
ibrier = fun_ibrier_score(pcr.brier_score)
print(ibrier)
```

##Partial Least square

```{r}
mod.pls = fun_train(train, fit = "PLS")
```


```{R}
pls.brier_score = pred_error(obj = mod.pls, fit = "PLS")
pls.roc = pred_error(obj = mod.pls, fit = "PLS", pred = "ROC")
tdROC::plot.tdROC(pls.roc)
```

##Lasso

Another method we have talked about is the penalised likelihood methods ridge regression and Lasso.

```{r}
mod.Lasso = fun_train(train, fit = "Lasso")
plot(mod.Lasso)
```

We can view the optimal $\lambda$ value. The left vertical line shows where the CV error hits its minimum. The right vertical line shows the most regularized model within 1 standard deviation of the minimum.

```{r}
Lasso.brier_score = pred_error(obj = mod.lasso, fit = "Lasso", pred = "Brier")

plot_brier(Lasso.brier_score)
```

```{r}
Lasso.ibrier = fun_ibrier_score(Lasso.brier_score)
print(Lasso.ibrier)
```

##time dependent ROC

```{r}
Lasso.roc = pred_error(obj = mod.Lasso, fit = "Lasso", pred = "ROC")
tdROC::plot.tdROC(Lasso.roc)
```
It is argued that survival trees might have better performance than other regression methods. We can fit a regression tree to our simulated data with:

```{r}
stree = fun_train(train, fit = "tree")
plot(stree)
```

Nevertheless, decision trees can create overcomplex trees that don't generalise well. On the other hand it has been argued that the correction method of Bonferroni is too restrictive in DNA microarray data. Instead, a regularise Univariate method with restricted maximum depth is used.

We have said that ensemble methods such as random forest may give better prediction:

```{r}
bst = fun_train(train, fit = "random forest")
```

Let's calculate the Brier score:
```{R}
bst.brier_score = pred_error(obj = bst, fit = "random forest", pred = "Brier")
plot_brier(bst.brier_score)
```

```{r}
ibrier = fun_ibrier_score(bst.brier_score)
print(ibrier)
```


Experimental feature in survival analysis:
```{R}
var_imp(bst)
```

Let's see a plot for more than one model:
```{R}
roc.plot2(Lasso.roc, uni.roc, fs.roc, pcr.roc)
```

```{r}
plot_brier2(Lasso.brier_score, uni.brier_score, fs.brier_score, pcr.brier_score, bst.brier_score)
```

Moreover we are interested in the prediction error for different training/test splits. Thus, we will compute the CV error as a measure of error.
```{r}
cv.ibrier.fs = fun_cv(data = survdata, model_selection = "Stepwise", K = 10) 
print(cv.ibrier.fs)
```

Now we can plot the predictions of the model using the function survfit:
```{r}
pred.fs.coxph = mod_pred(obj = mod.fs.coxph, beta = coef)
```

This looks like the forward Stepwise method is performing poorly, lets see a plot comparing this two:

```{r}
nx = as.matrix(test %>% dplyr::select(dplyr::contains("feature")))
pred <- predict(Lasso_cv_fit, newx = nx, s = "lambda.min")
```

```{r}
predict.coxph
ride {survival}
smod <- with(train %>%
               dplyr::mutate(os_deceased = os_status == "DECEASED")
               , Surv(os_months, os_deceased))
# integrated Brier score up to max(time)
ipred::sbrier(smod, pred)
```


Vignettes are long form documentation commonly included in packages. Because they are part of the distribution of the package, they need to be as compact as possible. The `html_vignette` output type provides a custom style sheet (and tweaks some options) to ensure that the resulting html is as small as possible. The `html_vignette` format:

- Never uses retina figures
- Has a smaller default figure size
- Uses a custom CSS stylesheet instead of the default Twitter Bootstrap style

## Vignette Info

Note the various macros within the `vignette` section of the metadata block above. These are required in order to instruct R how to build the vignette. Note that you should change the `title` field and the `\VignetteIndexEntry` to match the title of your vignette.

## Styles

The `html_vignette` template includes a basic CSS theme. To override this theme you can specify your own CSS in the document metadata as follows:

    output: 
      rmarkdown::html_vignette:
        css: mystyles.css

## Figures

The figure sizes have been customised so that you can easily put two images side-by-side. 

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```

You can enable figure captions by `fig_caption: yes` in YAML:

    output:
      rmarkdown::html_vignette:
        fig_caption: yes

Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**.

## More Examples

You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(mtcars, 10))
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
